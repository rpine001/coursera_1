{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of getHistFMP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1qz0YGGXuYV2cn7ZKp_i1cFO-hecIHqFl",
      "authorship_tag": "ABX9TyPwrS9d7vZ5nRa+PG92Xzg7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpine001/coursera_1/blob/master/Copy_of_getHistFMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4vikyolX-gQ",
        "outputId": "22921268-1703-482c-9a9d-3e740590569d"
      },
      "source": [
        "import requests\n",
        "import csv\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import linregress\n",
        "\n",
        "#https://www.the905guy.ca/article/how-to-create-a-relative-momentum-trading-strategy-in-python\n",
        "\n",
        "def get_eod_data(symbol, session=None):\n",
        "  if session is None:\n",
        "    session = requests.Session()\n",
        "    start = \"2019-09-1\"\n",
        "    end = \"2021-04-8\"\n",
        "    url = \"https://eodhistoricaldata.com/api/eod/%s\" % symbol\n",
        "    params = {\"api_token\": \"5c9ff10ca240a0.35985529\",\"from\":start,\"to\":end}\n",
        "    r = session.get(url, params=params)\n",
        "    if r.status_code == requests.codes.ok:\n",
        "      #r_text = StringIO(r.text)\n",
        "      #df = pd.read_csv(StringIO(r.text), skipfooter=1, parse_dates=[0], index_col=0, engine=\"python\") #####DONT do this since can't use function more than once\n",
        "      return StringIO(r.text) #r_text #df\n",
        "\n",
        "  else:raise Exception(r.status_code, r.reason, url)\n",
        "\n",
        "\n",
        "def momentum(closes):\n",
        "    ##use linear regression of returns to measure risk adjust returns\n",
        "    returns = np.log(closes)\n",
        "    ##create index equal to length of array\n",
        "    x = np.arange(len(returns))\n",
        "    slope, _, rvalue, _, _ = linregress(x, returns)\n",
        "    return ((1 + slope) ** 252) * (rvalue ** 2)  # annualize slope and multiply by R^2\n",
        "\n",
        "def slopePrice(closes):\n",
        "    ##use scipy.stats to calculate linear regression values\n",
        "    y = closes\n",
        "    x = np.arange(len(y))\n",
        "    slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
        "    return  intercept+slope*(t_lrch+lag_lrch)+std_err\n",
        "\n",
        "\n",
        "#pd.set_option('display.max_rows', None)\n",
        "\n",
        "#### Get list of assets\n",
        "\n",
        "\n",
        "\n",
        "#my_df = pd.read_csv('mf_testing_data.csv')\n",
        "\n",
        "#Get historic prices\n",
        "\n",
        "#stocks = ['NVS.US']\n",
        "#assetFile = \"testlist1.csv\"\n",
        "#assetFile = \"assets2.csv\"\n",
        "assetFile = \"assets2.csv\"\n",
        "#assetFile ='/content/drive/MyDrive/Colab Data/assets2.csv'\n",
        "stocks = pd.read_csv(assetFile, header=None)[0]\n",
        "#stocks = ['NVS.US','AAPL.US','MSFT.US','GOOG.US','XLU.US','TLT.US','SPY.US']\n",
        "#stocks = ['NVS.US','AAPL.US','SPY.US']\n",
        "vols = ['VIX.INDX','VIX3M.INDX','VIX6M.INDX','VIX1Y.INDX']\n",
        "\n",
        "rebal_freq=5\n",
        "field = \"Adjusted_close\"\n",
        "t_lrrsq = 60\n",
        "t_lrch = 60\n",
        "lag_lrch = 10\n",
        "\n",
        "empresas = {}\n",
        "empresas1 ={}\n",
        "empresas2 ={}\n",
        "empresasVIX={}\n",
        "#Get all prices into a dataframe\n",
        "for stock in stocks:\n",
        "  #prices = get_eod_data(stock)\n",
        "  prices = pd.read_csv(get_eod_data(stock), skipfooter=1, parse_dates=[0], index_col=0, engine=\"python\")\n",
        "  empresas1[stock] = prices[field].pct_change(1)\n",
        "  #empresas[stock] = prices.set_index('Date') NOTE: not required since data is already indexed on Date\n",
        "  #empresas[stock] = prices[\"Close\"].pct_change(1).rolling(1).sum()/2+prices[\"Close\"].pct_change().rolling(5).sum()/2\n",
        "  empresas[stock] = prices[field].pct_change(5)/4+prices[field].pct_change(20)/2*0+prices[field].pct_change(60)/4*0\n",
        "  #empresas[stock] = prices[field].rolling(t_lrrsq).apply(momentum, raw=False)\n",
        "  empresas2[stock] = prices[field].rolling(t_lrch).apply(slopePrice, raw=False)\n",
        "\n",
        "\n",
        "for v in vols:\n",
        "  v_series = pd.read_csv(get_eod_data(v), skipfooter=1, parse_dates=[0], index_col=0, engine=\"python\")\n",
        "  empresasVIX[v] = v_series[field]\n",
        "\n",
        "\n",
        "df_rtns = pd.concat(empresas1, axis=1) \n",
        "\n",
        "df_mmtm = pd.concat(empresas, axis=1)\n",
        "\n",
        "df_lrch = pd.concat(empresas2, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "df_vix = pd.concat(empresasVIX, axis=1)\n",
        "### Calc VIX filter: sig_VIX is when previous (VIX.INDX/VIX3M.INDX-1) > x%\n",
        "\n",
        "df_vix['sigRtns'] = df_vix['VIX3M.INDX'].shift(periods=1, axis = 0)-df_vix['VIX.INDX'].shift(periods=1, axis = 0)\n",
        "## add in calculation of zscore\n",
        "#df_vix['zVIX'] = \n",
        "#df_vix['sig_VIX'] = np.where(df_vix['VIX.INDX'].shift(periods=1, axis = 0)/df_vix['VIX3M.INDX'].shift(periods=1, axis = 0)-1>-0.15, 1,1 )\n",
        "df_vix['sig_VIX'] = np.where(df_vix['sigRtns']>-1, 1,0 )\n",
        "df_rank = pd.DataFrame(columns=df_mmtm.columns, index=df_mmtm.index)\n",
        "\n",
        "\n",
        "for i in range(len(df_rank)):\n",
        "# calculate rank only on rebal date, NOTE: here % is the modulus function, i % rebal = 0 whenever i is a multiple of rebal_freq\n",
        "  if i % rebal_freq == 0:\n",
        "    df_rank.iloc[i] = df_mmtm.iloc[i].rank(axis=0, ascending=False)\n",
        "\n",
        "#shift the rank observations to the next day - ie the day trade can occur\n",
        "#then fill in the NaN between ranking dates with previous ranking\n",
        "\n",
        "for i in range(len(df_vix)):\n",
        "### calculate the lin regress slope of the vix futures\n",
        "  x = np.arange(6)\n",
        "  testval= linregress(x, df_vix.iloc[i]).slope\n",
        "  #df_vix['slope'].iloc[i]= linregress(x, df_vix.iloc[i]).slope\n",
        "\n",
        "df_rank = df_rank.shift(periods=1, axis = 0)\n",
        "df_rank.ffill(inplace = True)\n",
        "num_long = 5\n",
        "num_short = 0\n",
        "num_stocks = df_rank.shape[1] - num_short + 1\n",
        "#allocate weight to top N and bottom N\n",
        "for col in df_rank.columns:\n",
        "    \n",
        "    # if a stock has a rank in the top num_long values, we go long\n",
        " \n",
        "    df_rank.loc[df_rank[col] <= num_long, col] = 1\n",
        "\n",
        "    # if a stock has a rank in the bottom num_short values, we go short\n",
        "    df_rank.loc[df_rank[col] >= num_stocks, col] = -0.5\n",
        "\n",
        "    # if a stock doesn't have a ranking in top or bottom n values, we have no position\n",
        "    df_rank.loc[(df_rank[col] < num_stocks) & (df_rank[col] > num_long), col] = None\n",
        "\n",
        "#get daily changes from original prices file and multiply by rank wgts\n",
        "df_wgtRtn = df_rtns*df_rank\n",
        "#divide wgt rtn by the number of stock bought and sold\n",
        "df_wgtRtn[\"Row Rtn\"] = df_wgtRtn.sum(axis=1, skipna=True)/(num_long+num_short) \n",
        "df_wgtRtn[\"rtnBM\"] =df_mmtm['SPY.US']\n",
        "df_wgtRtn['sigSPY'] = 1\n",
        "df_wgtRtn['Exit'] = np.where(df_mmtm['SPY.US'].shift(periods=1, axis = 0)>0, 1,1 )\n",
        "df_wgtRtn=pd.concat([df_wgtRtn, df_vix[['sig_VIX']]], axis=1, sort=False, join='inner')\n",
        "df_wgtRtn=pd.concat([df_wgtRtn, df_vix[['sigRtns']]], axis=1, sort=False, join='inner')\n",
        "df_wgtRtn[\"Portfolio Value\"] = (df_wgtRtn[\"Row Rtn\"]*df_wgtRtn['Exit']*df_wgtRtn['sig_VIX']+1).cumprod()*1\n",
        "df_wgtRtn[\"BMrk\"] =(df_rtns[\"SPY.US\"] +1).cumprod()*1\n",
        "\n",
        "\n",
        "\n",
        "#df_wgtRtn['Exit']\n",
        "#df_wgtRtn[\"rtnBM\"]\n",
        "df_wgtRtn['Portfolio Value'].plot()\n",
        "df_wgtRtn['BMrk'].plot()\n",
        "\n",
        "\n",
        "df_test = df_vix['VIX.INDX'].shift(periods=1, axis = 0)/df_vix['VIX3M.INDX'].shift(periods=1, axis = 0)-1\n",
        "#print (df_test)\n",
        "df_test.plot()\n",
        "#print (df_vix)\n",
        "\n",
        "plt.title('Equity Curve for Relative Momentum Strategy')\n",
        "plt.show()\n",
        "plt.title('Scatter')\n",
        "plt.plot(df_wgtRtn['sigRtns'],df_wgtRtn[\"SPY.US\"], 'o' )\n",
        "\n",
        "df_rank.to_csv ('/content/drive/MyDrive/Colab Data/rank.csv', index = True, header=True)\n",
        "\n",
        "print(df_lrch)  \n",
        "#return_stocks = portfolio.pct_change()\n",
        "#df_wgtRtn\n",
        "#bests\n",
        "#df_rtns\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-54ae4b0b74a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0massetFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"assets2.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#assetFile ='/content/drive/MyDrive/Colab Data/assets2.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mstocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massetFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#stocks = ['NVS.US','AAPL.US','MSFT.US','GOOG.US','XLU.US','TLT.US','SPY.US']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#stocks = ['NVS.US','AAPL.US','SPY.US']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'assets2.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OOEfLAPkAEg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8CGyC7RNPV"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRJbS30wIQjj"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}